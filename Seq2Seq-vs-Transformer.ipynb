{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12723479,"sourceType":"datasetVersion","datasetId":8041935}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key=\"fb4c8007ed0d1fb692b2279b11bb69081f2c698d\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-13T06:21:41.075681Z","iopub.execute_input":"2025-08-13T06:21:41.075921Z","iopub.status.idle":"2025-08-13T06:21:49.388251Z","shell.execute_reply.started":"2025-08-13T06:21:41.075903Z","shell.execute_reply":"2025-08-13T06:21:49.387582Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23c014\u001b[0m (\u001b[33mma23c014-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport wandb\nfrom tqdm import tqdm\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T06:22:04.180411Z","iopub.execute_input":"2025-08-13T06:22:04.180658Z","iopub.status.idle":"2025-08-13T06:22:10.352688Z","shell.execute_reply.started":"2025-08-13T06:22:04.180640Z","shell.execute_reply":"2025-08-13T06:22:10.351919Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Dataset utilities\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:10:45.556560Z","iopub.execute_input":"2025-08-13T05:10:45.557270Z","iopub.status.idle":"2025-08-13T05:10:45.562258Z","shell.execute_reply.started":"2025-08-13T05:10:45.557247Z","shell.execute_reply":"2025-08-13T05:10:45.561516Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for source, target in pairs:\n        input_chars.update(source)\n        output_chars.update(target)\n    input_vocab = {c: i + 1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i + 3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"target\", \"source\", \"count\"], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df[\"source\"], df[\"target\"]))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(seq) for seq in inputs]\n    target_lens = [len(seq) for seq in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        x = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_token, hidden):\n        x = self.embedding(input_token.unsqueeze(1))\n        output, hidden = self.rnn(x, hidden)\n        output = self.fc(output.squeeze(1))\n        return output, hidden\n\n    def beam_search(self, hidden, max_len, sos_idx, eos_idx, beam_size=3):\n        device = next(self.parameters()).device\n        sequences = [[torch.tensor([sos_idx], device=device), hidden, 0.0]]\n        completed = []\n\n        for _ in range(max_len):\n            new_sequences = []\n            for seq, h, score in sequences:\n                input_token = seq[-1].unsqueeze(0)\n                output, new_hidden = self.forward(input_token, h)\n                probs = torch.log_softmax(output, dim=-1).squeeze(0)\n                topk_probs, topk_indices = probs.topk(beam_size)\n                for i in range(beam_size):\n                    next_token = topk_indices[i].item()\n                    new_score = score + topk_probs[i].item()\n                    new_seq = torch.cat([seq, torch.tensor([next_token], device=device)])\n                    new_sequences.append([new_seq, new_hidden, new_score])\n            sequences = sorted(new_sequences, key=lambda x: x[2], reverse=True)[:beam_size]\n            completed.extend([seq for seq in sequences if seq[0][-1].item() == eos_idx])\n            sequences = [seq for seq in sequences if seq[0][-1].item() != eos_idx]\n            if not sequences:\n                break\n        completed = sorted(completed, key=lambda x: x[2], reverse=True)\n        return completed[0][0] if completed else sequences[0][0]\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        device = src.device\n        hidden = self.encoder(src, src_lens)\n        if tgt is not None:\n            tgt_len = tgt.size(1)\n            outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features, device=device)\n            input_token = tgt[:, 0]\n            for t in range(1, tgt_len):\n                output, hidden = self.decoder(input_token, hidden)\n                outputs[:, t] = output\n                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n                input_token = tgt[:, t] if teacher_force else output.argmax(1)\n            return outputs\n        else:\n            return [self.decoder.beam_search(hidden, max_len=20, sos_idx=1, eos_idx=2) for _ in range(batch_size)]\n\ndef accuracy(preds, targets, pad_idx=0):\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\ndef train(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_acc = 0, 0\n    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n        acc = accuracy(output[:, 1:], tgt[:, 1:])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_acc = 0, 0\n    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Evaluating\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n        output = model(src, src_lens, tgt, teacher_forcing_ratio=0.0)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n        acc = accuracy(output[:, 1:], tgt[:, 1:])\n        total_loss += loss.item()\n        total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\ndef main():\n    import wandb\n    # Run name will be assigned after wandb.init with config\n    def generate_run_name(config):\n        return f\"cell:{config.cell_type}_embed:{config.embed_size}_hid:{config.hidden_size}_layers:{config.num_layers}_beam:{config.beam_size}\"\n\n    # First initialize W&B run with placeholder name\n    wandb.init(project=\"Dakshina-Translitration\", config=wandb.config)\n    config = wandb.config\n\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n    decoder = Decoder(len(output_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(10):\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc\n        })\n\n\nif __name__ == \"__main__\":\n    sweep_config = {\n        \"method\": \"bayes\",\n        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"embed_size\": {\"values\": [32, 64, 128]},\n            \"hidden_size\": {\"values\": [64, 128, 256]},\n            \"num_layers\": {\"values\": [1,2,3]},\n            \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n            \"dropout\": {\"values\": [0.1,0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.01},\n            \"batch_size\": {\"values\": [16,32, 64]},\n            \"beam_size\": {\"values\": [1, 3, 5]}  \n        }\n    }\n\n    sweep_id = wandb.sweep(sweep_config, project=\"Dakshina-Translitration\")\n    wandb.agent(sweep_id, function=main, count=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:10:49.866054Z","iopub.execute_input":"2025-08-13T05:10:49.866335Z","iopub.status.idle":"2025-08-13T06:03:35.559884Z","shell.execute_reply.started":"2025-08-13T05:10:49.866317Z","shell.execute_reply":"2025-08-13T06:03:35.559134Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: dcxc1gbq\nSweep URL: https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hpzl9bt8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008439638735407979\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_051055-hpzl9bt8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/hpzl9bt8' target=\"_blank\">pious-sweep-1</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/hpzl9bt8' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/hpzl9bt8</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁█▇▇▆▆▆▆▆▆</td></tr><tr><td>train_loss</td><td>█▁▁▁▂▃▂▃▃▃</td></tr><tr><td>val_accuracy</td><td>▅▃▁▂▃▆▇█▄▇</td></tr><tr><td>val_loss</td><td>▂▅▅▅█▁▁▁█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.24876</td></tr><tr><td>train_loss</td><td>2.85048</td></tr><tr><td>val_accuracy</td><td>0.24265</td></tr><tr><td>val_loss</td><td>3.01167</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pious-sweep-1</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/hpzl9bt8' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/hpzl9bt8</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250813_051055-hpzl9bt8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0d27zwnx with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0002230722371866199\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_051548-0d27zwnx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/0d27zwnx' target=\"_blank\">gallant-sweep-2</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/0d27zwnx' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/0d27zwnx</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.70067</td></tr><tr><td>train_loss</td><td>0.96394</td></tr><tr><td>val_accuracy</td><td>0.62707</td></tr><tr><td>val_loss</td><td>1.21508</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">gallant-sweep-2</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/0d27zwnx' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/0d27zwnx</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250813_051548-0d27zwnx/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: avtb9obu with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0002528366344936001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_052612-avtb9obu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/avtb9obu' target=\"_blank\">stilted-sweep-3</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/avtb9obu' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/avtb9obu</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▆▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.70692</td></tr><tr><td>train_loss</td><td>0.9611</td></tr><tr><td>val_accuracy</td><td>0.60466</td></tr><tr><td>val_loss</td><td>1.33157</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stilted-sweep-3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/avtb9obu' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/avtb9obu</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250813_052612-avtb9obu/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i640d8ht with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006429198317861472\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_052858-i640d8ht</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/i640d8ht' target=\"_blank\">bumbling-sweep-4</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/i640d8ht' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/i640d8ht</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.80981</td></tr><tr><td>train_loss</td><td>0.62402</td></tr><tr><td>val_accuracy</td><td>0.68884</td></tr><tr><td>val_loss</td><td>1.06928</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bumbling-sweep-4</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/i640d8ht' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/i640d8ht</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250813_052858-i640d8ht/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rpmrgdvz with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0022888840303261935\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_053225-rpmrgdvz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/rpmrgdvz' target=\"_blank\">dazzling-sweep-5</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/rpmrgdvz' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/rpmrgdvz</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.89628</td></tr><tr><td>train_loss</td><td>0.34043</td></tr><tr><td>val_accuracy</td><td>0.72763</td></tr><tr><td>val_loss</td><td>1.10504</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dazzling-sweep-5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/rpmrgdvz' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/rpmrgdvz</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250813_053225-rpmrgdvz/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s7nh7ife with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0004257192410430172\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_053641-s7nh7ife</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/s7nh7ife' target=\"_blank\">soft-sweep-6</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/s7nh7ife' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/s7nh7ife</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.90909</td></tr><tr><td>train_loss</td><td>0.30215</td></tr><tr><td>val_accuracy</td><td>0.72681</td></tr><tr><td>val_loss</td><td>1.08722</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">soft-sweep-6</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/s7nh7ife' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/s7nh7ife</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250813_053641-s7nh7ife/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jrzvz8rr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009931367997747169\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_054409-jrzvz8rr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/jrzvz8rr' target=\"_blank\">quiet-sweep-7</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/jrzvz8rr' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/jrzvz8rr</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.88279</td></tr><tr><td>train_loss</td><td>0.38848</td></tr><tr><td>val_accuracy</td><td>0.7283</td></tr><tr><td>val_loss</td><td>1.05472</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">quiet-sweep-7</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/jrzvz8rr' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/jrzvz8rr</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250813_054409-jrzvz8rr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vcd0qutm with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0010829103250540263\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_055649-vcd0qutm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/vcd0qutm' target=\"_blank\">vital-sweep-8</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/dcxc1gbq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/vcd0qutm' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/vcd0qutm</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▆▇▇██▇▇▇</td></tr><tr><td>val_loss</td><td>▇▁▁▂▃▃▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.90959</td></tr><tr><td>train_loss</td><td>0.30068</td></tr><tr><td>val_accuracy</td><td>0.71301</td></tr><tr><td>val_loss</td><td>1.21903</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vital-sweep-8</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/vcd0qutm' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/vcd0qutm</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250813_055649-vcd0qutm/logs</code>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Test Data","metadata":{}},{"cell_type":"code","source":"# ---------------- Dataset & Utils ----------------\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(x) for x in inputs]\n    target_lens = [len(x) for x in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\n# ---------------- Models ----------------\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, token, hidden):\n        x = self.embedding(token.unsqueeze(1))\n        output, hidden = self.rnn(x, hidden)\n        output = self.fc(output.squeeze(1))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        hidden = self.encoder(src, src_lens)\n        tgt_len = tgt.size(1)\n        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n        input_token = tgt[:, 0]\n        for t in range(1, tgt_len):\n            output, hidden = self.decoder(input_token, hidden)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            input_token = tgt[:, t] if teacher_force else output.argmax(1)\n        return outputs\n\n# ---------------- Train + Eval ----------------\ndef train_model(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for src, tgt, src_lens, _ in dataloader:\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)\n        loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef evaluate_and_save(model, dataloader, input_vocab, output_vocab, device, csv_path=None):\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    correct = 0\n    total = 0\n    results = []\n\n    with torch.no_grad():\n        for src, tgt, src_lens, _ in dataloader:\n            src = src.to(device)\n            hidden = model.encoder(src, src_lens)\n            input_token = torch.tensor([output_vocab['<sos>']] * src.size(0)).to(device)\n            decoded = []\n            for _ in range(20):\n                output, hidden = model.decoder(input_token, hidden)\n                input_token = output.argmax(1)\n                decoded.append(input_token)\n            decoded = torch.stack(decoded, dim=1)\n\n            for i in range(src.size(0)):\n                pred = ''.join([inv_output_vocab[t.item()] for t in decoded[i] if t.item() not in [output_vocab['<eos>'], 0]])\n                truth = ''.join([inv_output_vocab[t.item()] for t in tgt[i][1:-1]])\n                inp = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n                results.append((inp, pred, truth))\n                if pred == truth:\n                    correct += 1\n                total += 1\n\n    acc = correct / total * 100\n    print(f\"\\n Test Accuracy: {acc:.2f}%\")\n    for inp, pred, truth in results[:10]:\n        print(f\"{inp:<15} | Pred: {pred:<20} | Truth: {truth}\")\n\n    if csv_path is not None:\n        with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n            writer = csv.writer(f)\n            writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n            writer.writerows(results)\n        print(f\"\\n Predictions saved to: {csv_path}\")\n\n    return acc, results\n\n\n# ------------ Run ----------------\nif __name__ == \"__main__\":\n    config = {\n        \"embed_size\": 128,\n        \"hidden_size\": 256,\n        \"num_layers\": 3,\n        \"cell_type\": \"LSTM\",\n        \"dropout\": 0.2,\n        \"batch_size\": 32,\n        \"lr\": 0.0004257192410430172,\n        \"epochs\": 10,\n    }\n\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_acc = 0\n    for epoch in range(config[\"epochs\"]):\n        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n        print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n        acc, results = evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=None)\n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n    print(\"\\n Loading best model for final evaluation...\")\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n\n    # Save predictions CSV here\n    evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T06:22:25.868374Z","iopub.execute_input":"2025-08-13T06:22:25.868861Z","iopub.status.idle":"2025-08-13T06:35:52.563292Z","shell.execute_reply.started":"2025-08-13T06:22:25.868838Z","shell.execute_reply":"2025-08-13T06:35:52.562588Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Train Loss: 1.9336\n\n Test Accuracy: 17.95%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंंडर                | Truth: अंकोर\nankor           | Pred: अंकर                 | Truth: अंकोर\nangaarak        | Pred: अंगराक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\nEpoch 2 Train Loss: 0.9420\n\n Test Accuracy: 26.41%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगोकर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\nEpoch 3 Train Loss: 0.7336\n\n Test Accuracy: 29.94%\nank             | Pred: एंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगोकर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\nEpoch 4 Train Loss: 0.6159\n\n Test Accuracy: 33.56%\nank             | Pred: एंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 5 Train Loss: 0.5344\n\n Test Accuracy: 34.76%\nank             | Pred: एंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनाकों               | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगोकर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगररक               | Truth: अंगारक\nEpoch 6 Train Loss: 0.4728\n\n Test Accuracy: 35.78%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगोकर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 7 Train Loss: 0.4186\n\n Test Accuracy: 37.12%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अनखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: एंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 8 Train Loss: 0.3738\n\n Test Accuracy: 38.05%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 9 Train Loss: 0.3342\n\n Test Accuracy: 38.36%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगारक               | Truth: अंगारक\nEpoch 10 Train Loss: 0.3016\n\n Test Accuracy: 36.41%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: एंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: एंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगारक               | Truth: अंगारक\n\n Loading best model for final evaluation...\n\n Test Accuracy: 38.36%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगारक               | Truth: अंगारक\n\n Predictions saved to: test_predictions.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"___\n___","metadata":{}},{"cell_type":"markdown","source":"#  **$$Transformer-Model$$**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport wandb\nfrom tqdm import tqdm\nimport csv\nimport math\nfrom collections import namedtuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T06:58:22.067875Z","iopub.execute_input":"2025-08-11T06:58:22.068119Z","iopub.status.idle":"2025-08-11T06:58:26.842157Z","shell.execute_reply.started":"2025-08-11T06:58:22.068102Z","shell.execute_reply":"2025-08-11T06:58:26.841408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport wandb\nfrom tqdm import tqdm\nimport math\nimport csv\nfrom collections import namedtuple\n\n# ---------------- Data Processing and Utilities ----------------\n\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n        # Robustly get unk indices, with fallbacks\n        self.unk_in = input_vocab.get('<unk>', 1) \n        self.unk_out = output_vocab.get('<unk>', 3)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        # Use .get() with unk_in/unk_out for handling unseen characters\n        input_ids = [self.input_vocab.get(c, self.unk_in) for c in source]\n        target_ids = [self.sos] + [self.output_vocab.get(c, self.unk_out) for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    \n    # Vocab indexing: <pad>:0, <unk>:1, then sorted chars\n    input_vocab = {c: i + 2 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    input_vocab['<unk>'] = 1\n    \n    # Vocab indexing: <pad>:0, <sos>:1, <eos>:2, <unk>:3, then sorted chars\n    output_vocab = {c: i + 4 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3})\n    \n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    # Ensure the path is correct for your environment (e.g., Kaggle, local, Colab)\n    # Common issue: FileNotFoundError if path is wrong.\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded\n\n# ---------------- Transformer Specific Components ----------------\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0) # Add batch dimension\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # Add positional encoding to input embeddings\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers,\n                 num_decoder_layers, dim_feedforward, dropout):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=0)\n        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model, padding_idx=0)\n        self.positional_encoding = PositionalEncoding(d_model, dropout)\n        \n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True # Important: Use batch_first for convenience\n        )\n        \n        self.fc_out = nn.Linear(d_model, output_vocab_size)\n        self.output_vocab_size = output_vocab_size\n        self.sos_idx = 1\n        self.eos_idx = 2\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n        # Embed and add positional encoding\n        src_embedded = self.positional_encoding(self.encoder_embedding(src) * math.sqrt(self.d_model))\n        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt) * math.sqrt(self.d_model))\n        \n        # Pass through Transformer layers\n        transformer_out = self.transformer(\n            src_embedded, tgt_embedded,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask\n        )\n        \n        # Linear layer to get vocabulary logits\n        output = self.fc_out(transformer_out)\n        return output\n\n    def generate_square_subsequent_mask(self, sz):\n        # Generates a mask to prevent attention to future tokens in the decoder\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def create_padding_mask(self, seq, pad_idx=0):\n        # Generates a boolean mask for padding tokens\n        return (seq == pad_idx)\n\n# ---------------- Training and Evaluation Functions ----------------\n\ndef accuracy(preds, targets, pad_idx=0):\n    # Calculates character-level accuracy, ignoring padding\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\n@torch.no_grad()\ndef evaluate_word_accuracy(model, dataloader, device, output_vocab):\n    model.eval()\n    correct_words = 0\n    total_words = 0\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    \n    for src, tgt in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        src_padding_mask = model.create_padding_mask(src).to(device)\n        batch_size = src.size(0)\n        max_len = 20 # Max length for generated output (could be dynamically set based on input length if needed)\n        \n        # Initialize decoder input with <sos> tokens for greedy decoding\n        generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n        \n        for t in range(max_len):\n            # Create masks for the current generated sequence length\n            tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n            tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n            \n            # Forward pass to get next token predictions\n            output = model(src, generated_tokens, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask, tgt_padding_mask=tgt_padding_mask)\n            \n            # Get the token with the highest probability\n            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n            \n            # Append the predicted token to the generated sequence\n            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n            \n            # Stop if all sequences in the batch have generated the <eos> token\n            if (next_token == model.eos_idx).all():\n                break\n\n        # Calculate word-level accuracy\n        for i in range(batch_size):\n            pred_seq = generated_tokens[i]\n            target_seq = tgt[i]\n            \n            # Find the first <eos> token to trim the sequence (excluding <sos> and <eos> itself)\n            pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            \n            # Extract the actual word tokens, excluding <sos> and <eos>\n            pred_word = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n            target_word = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n            if torch.equal(pred_word, target_word):\n                correct_words += 1\n            total_words += 1\n            \n    return correct_words / total_words if total_words > 0 else 0.0\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_char_acc = 0, 0\n    for src, tgt in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        optimizer.zero_grad()\n        \n        src_padding_mask = model.create_padding_mask(src).to(device)\n        \n        tgt_input = tgt[:, :-1] # Input for decoder, excludes the last token\n        tgt_output = tgt[:, 1:]  # Target for loss, excludes the first token (<sos>)\n\n        # ************ CRITICAL FIX ************\n        # Create tgt_padding_mask from tgt_input to match its length\n        tgt_padding_mask = model.create_padding_mask(tgt_input).to(device)\n       \n        \n        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n        \n        output = model(src, tgt_input, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_mask=tgt_mask)\n        \n        # Reshape output and target for CrossEntropyLoss\n        loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        char_acc = accuracy(output, tgt_output) # Character-level accuracy\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_char_acc += char_acc\n        \n    return total_loss / len(loader), total_char_acc / len(loader)\n\ndef generate_predictions_csv(model, dataloader, input_vocab, output_vocab, device, csv_path):\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    results = []\n\n    with torch.no_grad():\n        for src, tgt in tqdm(dataloader, desc=\"Generating Test Predictions\"):\n            src = src.to(device)\n            batch_size = src.size(0)\n            max_len = 20 # Max length for generated output\n\n            # Inference loop for the decoder (similar to evaluate_word_accuracy)\n            generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n            \n            for t in range(max_len):\n                tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n                tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n                \n                output = model(src, generated_tokens, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask, tgt_padding_mask=tgt_padding_mask)\n                \n                next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n                generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n                \n                if (next_token == model.eos_idx).all():\n                    break\n\n            for i in range(batch_size):\n                pred_seq = generated_tokens[i]\n                target_seq = tgt[i]\n                \n                pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n                target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n                \n                pred_word_tokens = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n                # Ensure truth_word_tokens also excludes any potential padding if it's shorter than predicted length\n                truth_word_tokens = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n                pred_str = ''.join([inv_output_vocab[t.item()] for t in pred_word_tokens if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                truth_str = ''.join([inv_output_vocab[t.item()] for t in truth_word_tokens if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                inp_str = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n                results.append((inp_str, pred_str, truth_str))\n    \n    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n        writer.writerows(results)\n    print(f\"\\nPredictions saved to: {csv_path}\")\n\n# ---------------- Main Function for W&B Sweep ----------------\n\ndef main():\n    import wandb\n    \n    def generate_run_name(config):\n        return f\"transformer_d:{config.d_model}_nhead:{config.nhead}_layers:{config.num_encoder_layers}\"\n\n    wandb.init(project=\"Dakshina-Translitration-Transformer\", config=wandb.config)\n    config = wandb.config\n    wandb.run.name = generate_run_name(config)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    # *******************************************************************\n\n    # Build vocab on train + dev pairs for consistency\n    input_vocab, output_vocab = build_vocab(train_pairs + dev_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab) # Prepare test dataset here\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn) # Batch size 1 for individual prediction\n\n    model = TransformerModel(\n        input_vocab_size=len(input_vocab),\n        output_vocab_size=len(output_vocab),\n        d_model=config.d_model,\n        nhead=config.nhead,\n        num_encoder_layers=config.num_encoder_layers,\n        num_decoder_layers=config.num_decoder_layers,\n        dim_feedforward=config.dim_feedforward,\n        dropout=config.dropout\n    ).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=0) # ignore_index=0 for <pad> token\n\n    best_dev_acc = 0\n    # Training loop\n    for epoch in range(10): -\n        train_loss, train_char_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n        dev_word_acc = evaluate_word_accuracy(model, dev_loader, device, output_vocab)\n        \n        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Char Acc: {train_char_acc:.4f} | Dev Word Acc: {dev_word_acc:.4f}\")\n        \n        if dev_word_acc > best_dev_acc:\n            best_dev_acc = dev_word_acc\n            torch.save(model.state_dict(), 'best_transformer_model.pth')\n            print(f\" -> New best model saved with dev word accuracy: {best_dev_acc:.4f}\")\n\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_char_accuracy\": train_char_acc,\n            \"dev_word_accuracy\": dev_word_acc\n        })\n\n    print(\"\\nTraining complete. Loading best model for final evaluation on test set...\")\n    # Load the best model found during training\n    try:\n        model.load_state_dict(torch.load('best_transformer_model.pth'))\n    except FileNotFoundError:\n        print(\"Error: 'best_transformer_model.pth' not found. Ensure training completed successfully and model was saved.\")\n        return # Exit main if model not found\n\n    # Final evaluation on the test set (using the best saved model)\n    final_test_word_acc = evaluate_word_accuracy(model, test_loader, device, output_vocab)\n    print(f\"\\n--- Final Test Set Evaluation Results ---\")\n    print(f\"Word-level Accuracy on Test Set: {final_test_word_acc:.4f}\")\n    \n    # Generate and save predictions to CSV using the best model\n    generate_predictions_csv(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")\n    print(\"Test predictions saved to test_predictions.csv\")\n\n\nif __name__ == \"__main__\":\n    # Define your W&B sweep configuration\n    sweep_config = {\n        \"method\": \"bayes\", # Bayesian optimization\n        \"metric\": {\"name\": \"dev_word_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"d_model\": {\"values\": [128, 256, 512]},\n            \"nhead\": {\"values\": [4, 8, 16]},\n            \"num_encoder_layers\": {\"values\": [2, 4]},\n            \"num_decoder_layers\": {\"values\": [2, 4]},\n            \"dim_feedforward\": {\"values\": [512, 1024, 2048]},\n            \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.001},\n            \"batch_size\": {\"values\": [16, 32, 64]}\n        }\n    }\n    \n    # Initialize and run the W&B agent\n    sweep_id = wandb.sweep(sweep_config, project=\"Dakshina-Translitration-Transformer\")\n    wandb.agent(sweep_id, function=main, count=5) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T08:36:08.871755Z","iopub.execute_input":"2025-08-11T08:36:08.872040Z","iopub.status.idle":"2025-08-11T09:54:05.114951Z","shell.execute_reply.started":"2025-08-11T08:36:08.872018Z","shell.execute_reply":"2025-08-11T09:54:05.114407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport wandb\nfrom tqdm import tqdm\nimport math\nimport csv\nfrom collections import namedtuple\n\n# ---------------- Data Processing and Utilities ----------------\n\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n        self.unk_in = input_vocab.get('<unk>', 1)\n        self.unk_out = output_vocab.get('<unk>', 3)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab.get(c, self.unk_in) for c in source]\n        target_ids = [self.sos] + [self.output_vocab.get(c, self.unk_out) for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    \n    input_vocab = {c: i + 2 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    input_vocab['<unk>'] = 1\n    \n    output_vocab = {c: i + 4 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3})\n    \n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded\n\n# ---------------- Transformer Specific Components ----------------\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers,\n                 num_decoder_layers, dim_feedforward, dropout):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=0)\n        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model, padding_idx=0)\n        self.positional_encoding = PositionalEncoding(d_model, dropout)\n        \n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.fc_out = nn.Linear(d_model, output_vocab_size)\n        self.output_vocab_size = output_vocab_size\n        self.sos_idx = 1\n        self.eos_idx = 2\n\n    # Corrected forward method signature\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n        src_embedded = self.positional_encoding(self.encoder_embedding(src) * math.sqrt(self.d_model))\n        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt) * math.sqrt(self.d_model))\n        \n        transformer_out = self.transformer(\n            src_embedded, tgt_embedded,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask\n        )\n        \n        output = self.fc_out(transformer_out)\n        return output\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def create_padding_mask(self, seq, pad_idx=0):\n        return (seq == pad_idx)\n\n# ---------------- Training and Evaluation Functions ----------------\n\ndef accuracy(preds, targets, pad_idx=0):\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\n@torch.no_grad()\ndef evaluate_word_accuracy(model, dataloader, device, output_vocab):\n    model.eval()\n    correct_words = 0\n    total_words = 0\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    \n    for src, tgt in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        src_padding_mask = model.create_padding_mask(src).to(device)\n        batch_size = src.size(0)\n        max_len = 20\n        \n        generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n        \n        for t in range(max_len):\n            tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n            tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n            \n            # Corrected keyword arguments\n            output = model(src, generated_tokens, \n                           src_key_padding_mask=src_padding_mask, \n                           tgt_key_padding_mask=tgt_padding_mask, \n                           tgt_mask=tgt_mask)\n            \n            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n            \n            if (next_token == model.eos_idx).all():\n                break\n\n        for i in range(batch_size):\n            pred_seq = generated_tokens[i]\n            target_seq = tgt[i]\n            \n            pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            \n            pred_word = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n            target_word = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n            if torch.equal(pred_word, target_word):\n                correct_words += 1\n            total_words += 1\n            \n    return correct_words / total_words if total_words > 0 else 0.0\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_char_acc = 0, 0\n    for src, tgt in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        optimizer.zero_grad()\n        \n        src_padding_mask = model.create_padding_mask(src).to(device)\n        tgt_input = tgt[:, :-1]\n        \n        tgt_padding_mask = model.create_padding_mask(tgt_input).to(device)\n        \n        tgt_output = tgt[:, 1:]\n\n        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n        \n        # Corrected keyword arguments\n        output = model(src, tgt_input, \n                       src_key_padding_mask=src_padding_mask, \n                       tgt_key_padding_mask=tgt_padding_mask, \n                       tgt_mask=tgt_mask)\n        \n        loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        char_acc = accuracy(output, tgt_output)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_char_acc += char_acc\n        \n    return total_loss / len(loader), total_char_acc / len(loader)\n\ndef generate_predictions_csv(model, dataloader, input_vocab, output_vocab, device, csv_path):\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    results = []\n\n    with torch.no_grad():\n        for src, tgt in tqdm(dataloader, desc=\"Generating Test Predictions\"):\n            src = src.to(device)\n            batch_size = src.size(0)\n            max_len = 20\n\n            src_padding_mask = model.create_padding_mask(src).to(device)\n            generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n            \n            for t in range(max_len):\n                tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n                tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n                \n                # Corrected keyword arguments\n                output = model(src, generated_tokens, \n                               src_key_padding_mask=src_padding_mask, \n                               tgt_key_padding_mask=tgt_padding_mask, \n                               tgt_mask=tgt_mask)\n                \n                next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n                generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n                \n                if (next_token == model.eos_idx).all():\n                    break\n\n            for i in range(batch_size):\n                pred_seq = generated_tokens[i]\n                target_seq = tgt[i]\n                \n                pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n                target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n                \n                pred_word_tokens = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n                truth_word_tokens = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n                pred_str = ''.join([inv_output_vocab[t.item()] for t in pred_word_tokens if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                truth_str = ''.join([inv_output_vocab[t.item()] for t in truth_word_tokens if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                inp_str = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n                results.append((inp_str, pred_str, truth_str))\n    \n    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n        writer.writerows(results)\n    print(f\"\\nPredictions saved to: {csv_path}\")\n\n# ---------------- Main Function for W&B Sweep ----------------\n\ndef main():\n    import wandb\n    \n    def generate_run_name(config):\n        return f\"transformer_d:{config.d_model}_nhead:{config.nhead}_layers:{config.num_encoder_layers}\"\n\n    wandb.init(project=\"Dakshina-Translitration-Transformer\", config=wandb.config)\n    config = wandb.config\n    wandb.run.name = generate_run_name(config)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n\n    # Build vocab on train + dev pairs for consistency\n    input_vocab, output_vocab = build_vocab(train_pairs + dev_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    model = TransformerModel(\n        input_vocab_size=len(input_vocab),\n        output_vocab_size=len(output_vocab),\n        d_model=config.d_model,\n        nhead=config.nhead,\n        num_encoder_layers=config.num_encoder_layers,\n        num_decoder_layers=config.num_decoder_layers,\n        dim_feedforward=config.dim_feedforward,\n        dropout=config.dropout\n    ).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_dev_acc = 0\n    # Training loop\n    for epoch in range(8):\n        train_loss, train_char_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n        dev_word_acc = evaluate_word_accuracy(model, dev_loader, device, output_vocab)\n        \n        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Char Acc: {train_char_acc:.4f} | Dev Word Acc: {dev_word_acc:.4f}\")\n        \n        if dev_word_acc > best_dev_acc:\n            best_dev_acc = dev_word_acc\n            # Save the model with a unique filename using the W&B run ID\n            model_path = f'best_transformer_model_{wandb.run.id}.pth'\n            torch.save(model.state_dict(), model_path)\n            print(f\" -> New best model saved to {model_path} with dev word accuracy: {best_dev_acc:.4f}\")\n\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_char_accuracy\": train_char_acc,\n            \"dev_word_accuracy\": dev_word_acc\n        })\n\n    print(\"\\nTraining complete. Loading best model for final evaluation on test set...\")\n    \n    # Load the best model found during this run\n    try:\n        model_path = f'best_transformer_model_{wandb.run.id}.pth'\n        model.load_state_dict(torch.load(model_path))\n    except FileNotFoundError:\n        print(\"Error: 'best_transformer_model.pth' not found. Ensure training completed successfully and model was saved.\")\n        return\n\n    final_test_word_acc = evaluate_word_accuracy(model, test_loader, device, output_vocab)\n    print(f\"\\n--- Final Test Set Evaluation Results ---\")\n    print(f\"Word-level Accuracy on Test Set: {final_test_word_acc:.4f}\")\n    \n    generate_predictions_csv(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")\n    print(\"Test predictions saved to test_predictions.csv\")\n\nif __name__ == \"__main__\":\n    sweep_config = {\n        \"method\": \"bayes\",\n        \"metric\": {\"name\": \"dev_word_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"d_model\": {\"values\": [128, 256, 512]},\n            \"nhead\": {\"values\": [4, 8, 16]},\n            \"num_encoder_layers\": {\"values\": [2, 4]},\n            \"num_decoder_layers\": {\"values\": [2, 4]},\n            \"dim_feedforward\": {\"values\": [512, 1024, 2048]},\n            \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.001},\n            \"batch_size\": {\"values\": [16, 32, 64]}\n        }\n    }\n    \n    sweep_id = wandb.sweep(sweep_config, project=\"Dakshina-Translitration-Transformer\")\n    wandb.agent(sweep_id, function=main, count=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T07:11:49.473987Z","iopub.execute_input":"2025-08-12T07:11:49.474228Z","execution_failed":"2025-08-12T09:12:17.062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For Test data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport math\nimport csv\nfrom collections import namedtuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T09:15:57.845954Z","iopub.execute_input":"2025-08-12T09:15:57.846267Z","execution_failed":"2025-08-12T09:28:56.304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------- Data Processing and Utilities ----------------\n\nclass TransliterationDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset for transliteration data.\n    \"\"\"\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n        self.unk_in = input_vocab.get('<unk>', 1)\n        self.unk_out = output_vocab.get('<unk>', 3)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab.get(c, self.unk_in) for c in source]\n        target_ids = [self.sos] + [self.output_vocab.get(c, self.unk_out) for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef build_vocab(pairs):\n    \"\"\"\n    Builds character-level vocabularies from a list of (source, target) pairs.\n    \"\"\"\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    \n    input_vocab = {c: i + 2 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    input_vocab['<unk>'] = 1\n    \n    output_vocab = {c: i + 4 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3})\n    \n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    \"\"\"\n    Loads transliteration pairs from a TSV file.\n    \"\"\"\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef collate_fn(batch):\n    \"\"\"\n    Pads sequences in a batch to the same length.\n    \"\"\"\n    inputs, targets = zip(*batch)\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded\n\n# ---------------- Transformer Specific Components ----------------\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Injects positional information into the input embeddings.\n    \"\"\"\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    \"\"\"\n    The main Transformer model for sequence-to-sequence transliteration.\n    \"\"\"\n    def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers,\n                 num_decoder_layers, dim_feedforward, dropout):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=0)\n        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model, padding_idx=0)\n        self.positional_encoding = PositionalEncoding(d_model, dropout)\n        \n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.fc_out = nn.Linear(d_model, output_vocab_size)\n        self.output_vocab_size = output_vocab_size\n        self.sos_idx = 1\n        self.eos_idx = 2\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n        src_embedded = self.positional_encoding(self.encoder_embedding(src) * math.sqrt(self.d_model))\n        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt) * math.sqrt(self.d_model))\n        \n        transformer_out = self.transformer(\n            src_embedded, tgt_embedded,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask\n        )\n        \n        output = self.fc_out(transformer_out)\n        return output\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def create_padding_mask(self, seq, pad_idx=0):\n        return (seq == pad_idx)\n\n# ---------------- Training and Evaluation Functions ----------------\n\ndef accuracy(preds, targets, pad_idx=0):\n    \"\"\"\n    Calculates character-level accuracy, ignoring padding tokens.\n    \"\"\"\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\n@torch.no_grad()\ndef evaluate_and_sample(model, dataloader, device, input_vocab, output_vocab, num_samples=10):\n    \"\"\"\n    Evaluates the model's word-level accuracy and returns a sample of predictions.\n    \"\"\"\n    model.eval()\n    correct_words = 0\n    total_words = 0\n    \n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    sample_predictions = []\n    samples_collected = 0\n\n    for src, tgt in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        src_padding_mask = model.create_padding_mask(src).to(device)\n        batch_size = src.size(0)\n        max_len = 20\n        \n        generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n        \n        for t in range(max_len):\n            tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n            tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n            \n            output = model(src, generated_tokens, \n                           src_key_padding_mask=src_padding_mask, \n                           tgt_key_padding_mask=tgt_padding_mask, \n                           tgt_mask=tgt_mask)\n            \n            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n            \n            if (next_token == model.eos_idx).all():\n                break\n\n        for i in range(batch_size):\n            pred_seq = generated_tokens[i]\n            target_seq = tgt[i]\n            \n            pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            \n            pred_word = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n            target_word = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n            if torch.equal(pred_word, target_word):\n                correct_words += 1\n            total_words += 1\n            \n            if samples_collected < num_samples:\n                inp_str = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n                pred_str = ''.join([inv_output_vocab.get(t.item(), '<unk>') for t in pred_word if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                truth_str = ''.join([inv_output_vocab.get(t.item(), '<unk>') for t in target_word if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                sample_predictions.append((inp_str, pred_str, truth_str))\n                samples_collected += 1\n            \n    return correct_words / total_words if total_words > 0 else 0.0, sample_predictions\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    \"\"\"\n    Trains the model for one epoch.\n    \"\"\"\n    model.train()\n    total_loss, total_char_acc = 0, 0\n    for src, tgt in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        optimizer.zero_grad()\n        \n        src_padding_mask = model.create_padding_mask(src).to(device)\n        tgt_input = tgt[:, :-1]\n        \n        tgt_padding_mask = model.create_padding_mask(tgt_input).to(device)\n        \n        tgt_output = tgt[:, 1:]\n\n        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n        \n        output = model(src, tgt_input, \n                       src_key_padding_mask=src_padding_mask, \n                       tgt_key_padding_mask=tgt_padding_mask, \n                       tgt_mask=tgt_mask)\n        \n        loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        char_acc = accuracy(output, tgt_output)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_char_acc += char_acc\n        \n    return total_loss / len(loader), total_char_acc / len(loader)\n\ndef generate_predictions_csv(model, dataloader, input_vocab, output_vocab, device, csv_path):\n    \"\"\"\n    Generates predictions for a test set and saves them to a CSV file.\n    \"\"\"\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    results = []\n\n    with torch.no_grad():\n        for src, tgt in tqdm(dataloader, desc=\"Generating Test Predictions\"):\n            src = src.to(device)\n            batch_size = src.size(0)\n            max_len = 20\n\n            src_padding_mask = model.create_padding_mask(src).to(device)\n            generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n            \n            for t in range(max_len):\n                tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n                tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n                \n                output = model(src, generated_tokens, \n                               src_key_padding_mask=src_padding_mask, \n                               tgt_key_padding_mask=tgt_padding_mask, \n                               tgt_mask=tgt_mask)\n                \n                next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n                generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n                \n                if (next_token == model.eos_idx).all():\n                    break\n\n            for i in range(batch_size):\n                pred_seq = generated_tokens[i]\n                target_seq = tgt[i]\n                \n                pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n                target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n                \n                pred_word_tokens = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n                truth_word_tokens = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n                pred_str = ''.join([inv_output_vocab.get(t.item(), '<unk>') for t in pred_word_tokens if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                truth_str = ''.join([inv_output_vocab.get(t.item(), '<unk>') for t in truth_word_tokens if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                inp_str = ''.join([inv_input_vocab.get(t.item(), '<unk>') for t in src[i] if t.item() != 0])\n                results.append((inp_str, pred_str, truth_str))\n    \n    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n        writer.writerows(results)\n    print(f\"\\nPredictions saved to: {csv_path}\")\n\n# ---------------- Main Function ----------------\n\ndef main():\n    # Model Hyperparameters\n    # These are fixed values for a simple run.\n    # You can change them to explore different configurations.\n    config = namedtuple(\"Config\", [\n        \"d_model\", \"nhead\", \"num_encoder_layers\", \"num_decoder_layers\",\n        \"dim_feedforward\", \"dropout\", \"lr\", \"batch_size\", \"num_epochs\"\n    ])(\n        d_model=256,\n        nhead=4,\n        num_encoder_layers=4,\n        num_decoder_layers=2,\n        dim_feedforward=1024,\n        dropout=0.1,\n        lr=0.0005,\n        batch_size=32,\n        num_epochs=10\n    )\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Load data\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n\n    # Build vocab on train + dev pairs for consistency\n    input_vocab, output_vocab = build_vocab(train_pairs + dev_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    model = TransformerModel(\n        input_vocab_size=len(input_vocab),\n        output_vocab_size=len(output_vocab),\n        d_model=config.d_model,\n        nhead=config.nhead,\n        num_encoder_layers=config.num_encoder_layers,\n        num_decoder_layers=config.num_decoder_layers,\n        dim_feedforward=config.dim_feedforward,\n        dropout=config.dropout\n    ).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_dev_acc = 0\n    best_model_path = 'best_transformer_model.pth'\n\n    # Training loop\n    for epoch in range(config.num_epochs):\n        train_loss, _ = train_epoch(model, train_loader, optimizer, criterion, device)\n        dev_word_acc, dev_samples = evaluate_and_sample(model, dev_loader, device, input_vocab, output_vocab, num_samples=10)\n        \n        print(f\"\\nEpoch {epoch+1} Train Loss: {train_loss:.4f}\\n\")\n        print(f\" Test Accuracy: {dev_word_acc:.2%}\")\n        for inp, pred, truth in dev_samples:\n            print(f\"{inp:<15}| Pred: {pred:<20}| Truth: {truth}\")\n        \n        if dev_word_acc > best_dev_acc:\n            best_dev_acc = dev_word_acc\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"\\n -> New best model saved to {best_model_path} with dev word accuracy: {best_dev_acc:.4f}\")\n\n    print(\"\\n Loading best model for final evaluation...\")\n    \n    # Load the best model found during this run\n    try:\n        model.load_state_dict(torch.load(best_model_path))\n    except FileNotFoundError:\n        print(\"Error: Best model checkpoint not found. Using the last trained model.\")\n        \n    final_test_word_acc, test_samples = evaluate_and_sample(model, test_loader, device, input_vocab, output_vocab, num_samples=10)\n    print(f\"\\n Final Test Accuracy: {final_test_word_acc:.2%}\")\n    for inp, pred, truth in test_samples:\n        print(f\"{inp:<15}| Pred: {pred:<20}| Truth: {truth}\")\n    \n    generate_predictions_csv(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}