{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12723479,"sourceType":"datasetVersion","datasetId":8041935}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key=\"fb4c8007ed0d1fb692b2279b11bb69081f2c698d\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-11T04:32:49.680355Z","iopub.execute_input":"2025-08-11T04:32:49.680590Z","iopub.status.idle":"2025-08-11T04:32:57.948826Z","shell.execute_reply.started":"2025-08-11T04:32:49.680566Z","shell.execute_reply":"2025-08-11T04:32:57.948278Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23c014\u001b[0m (\u001b[33mma23c014-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport wandb\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T04:33:04.542236Z","iopub.execute_input":"2025-08-11T04:33:04.542496Z","iopub.status.idle":"2025-08-11T04:33:08.374416Z","shell.execute_reply.started":"2025-08-11T04:33:04.542476Z","shell.execute_reply":"2025-08-11T04:33:08.373798Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Dataset utilities\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T04:33:11.464316Z","iopub.execute_input":"2025-08-11T04:33:11.465354Z","iopub.status.idle":"2025-08-11T04:33:11.471181Z","shell.execute_reply.started":"2025-08-11T04:33:11.465318Z","shell.execute_reply":"2025-08-11T04:33:11.470229Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for source, target in pairs:\n        input_chars.update(source)\n        output_chars.update(target)\n    input_vocab = {c: i + 1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i + 3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"target\", \"source\", \"count\"], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df[\"source\"], df[\"target\"]))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(seq) for seq in inputs]\n    target_lens = [len(seq) for seq in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        x = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_token, hidden):\n        x = self.embedding(input_token.unsqueeze(1))\n        output, hidden = self.rnn(x, hidden)\n        output = self.fc(output.squeeze(1))\n        return output, hidden\n\n    def beam_search(self, hidden, max_len, sos_idx, eos_idx, beam_size=3):\n        device = next(self.parameters()).device\n        sequences = [[torch.tensor([sos_idx], device=device), hidden, 0.0]]\n        completed = []\n\n        for _ in range(max_len):\n            new_sequences = []\n            for seq, h, score in sequences:\n                input_token = seq[-1].unsqueeze(0)\n                output, new_hidden = self.forward(input_token, h)\n                probs = torch.log_softmax(output, dim=-1).squeeze(0)\n                topk_probs, topk_indices = probs.topk(beam_size)\n                for i in range(beam_size):\n                    next_token = topk_indices[i].item()\n                    new_score = score + topk_probs[i].item()\n                    new_seq = torch.cat([seq, torch.tensor([next_token], device=device)])\n                    new_sequences.append([new_seq, new_hidden, new_score])\n            sequences = sorted(new_sequences, key=lambda x: x[2], reverse=True)[:beam_size]\n            completed.extend([seq for seq in sequences if seq[0][-1].item() == eos_idx])\n            sequences = [seq for seq in sequences if seq[0][-1].item() != eos_idx]\n            if not sequences:\n                break\n        completed = sorted(completed, key=lambda x: x[2], reverse=True)\n        return completed[0][0] if completed else sequences[0][0]\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        device = src.device\n        hidden = self.encoder(src, src_lens)\n        if tgt is not None:\n            tgt_len = tgt.size(1)\n            outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features, device=device)\n            input_token = tgt[:, 0]\n            for t in range(1, tgt_len):\n                output, hidden = self.decoder(input_token, hidden)\n                outputs[:, t] = output\n                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n                input_token = tgt[:, t] if teacher_force else output.argmax(1)\n            return outputs\n        else:\n            return [self.decoder.beam_search(hidden, max_len=20, sos_idx=1, eos_idx=2) for _ in range(batch_size)]\n\ndef accuracy(preds, targets, pad_idx=0):\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\ndef train(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_acc = 0, 0\n    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n        acc = accuracy(output[:, 1:], tgt[:, 1:])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_acc = 0, 0\n    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Evaluating\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n        output = model(src, src_lens, tgt, teacher_forcing_ratio=0.0)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n        acc = accuracy(output[:, 1:], tgt[:, 1:])\n        total_loss += loss.item()\n        total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\ndef main():\n    import wandb\n    # Run name will be assigned after wandb.init with config\n    def generate_run_name(config):\n        return f\"cell:{config.cell_type}_embed:{config.embed_size}_hid:{config.hidden_size}_layers:{config.num_layers}_beam:{config.beam_size}\"\n\n    # First initialize W&B run with placeholder name\n    wandb.init(project=\"Dakshina-Translitration\", config=wandb.config)\n    config = wandb.config\n\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n    decoder = Decoder(len(output_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(10):\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc\n        })\n\n\nif __name__ == \"__main__\":\n    sweep_config = {\n        \"method\": \"bayes\",\n        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"embed_size\": {\"values\": [32, 64, 128]},\n            \"hidden_size\": {\"values\": [64, 128, 256]},\n            \"num_layers\": {\"values\": [1,2,3]},\n            \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n            \"dropout\": {\"values\": [0.1,0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.01},\n            \"batch_size\": {\"values\": [16,32, 64]},\n            \"beam_size\": {\"values\": [1, 3, 5]}  \n        }\n    }\n\n    sweep_id = wandb.sweep(sweep_config, project=\"Dakshina-Translitration\")\n    wandb.agent(sweep_id, function=main, count=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T04:33:16.404401Z","iopub.execute_input":"2025-08-11T04:33:16.404902Z","iopub.status.idle":"2025-08-11T05:08:40.340731Z","shell.execute_reply.started":"2025-08-11T04:33:16.404879Z","shell.execute_reply":"2025-08-11T05:08:40.340053Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: 2tyssrla\nSweep URL: https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 17tszmhk with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001304138743761311\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_043322-17tszmhk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/17tszmhk' target=\"_blank\">grateful-sweep-1</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/17tszmhk' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/17tszmhk</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.72436</td></tr><tr><td>train_loss</td><td>0.91199</td></tr><tr><td>val_accuracy</td><td>0.61568</td></tr><tr><td>val_loss</td><td>1.33007</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">grateful-sweep-1</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/17tszmhk' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/17tszmhk</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250811_043322-17tszmhk/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kr1oziud with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008928603235359364\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_043615-kr1oziud</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kr1oziud' target=\"_blank\">leafy-sweep-2</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kr1oziud' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kr1oziud</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▃▅▁▂▅▇█▇▇▇</td></tr><tr><td>train_loss</td><td>▇▄█▇▄▂▁▁▂▂</td></tr><tr><td>val_accuracy</td><td>▆▄▁▃▆▆█▇▇▇</td></tr><tr><td>val_loss</td><td>▄▅█▆▂▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.61779</td></tr><tr><td>train_loss</td><td>1.23448</td></tr><tr><td>val_accuracy</td><td>0.54762</td></tr><tr><td>val_loss</td><td>1.50012</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">leafy-sweep-2</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kr1oziud' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kr1oziud</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250811_043615-kr1oziud/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gtm2ijqi with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.007252656727280787\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_043931-gtm2ijqi</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/gtm2ijqi' target=\"_blank\">lilac-sweep-3</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/gtm2ijqi' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/gtm2ijqi</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▄▇██▇▇▇▇▇</td></tr><tr><td>train_loss</td><td>█▄▂▁▁▂▂▂▂▂</td></tr><tr><td>val_accuracy</td><td>▁▄▅█▆▂▄▆▅▄</td></tr><tr><td>val_loss</td><td>█▄▃▁▃▅▃▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.28978</td></tr><tr><td>train_loss</td><td>2.71986</td></tr><tr><td>val_accuracy</td><td>0.24745</td></tr><tr><td>val_loss</td><td>2.9461</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lilac-sweep-3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/gtm2ijqi' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/gtm2ijqi</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250811_043931-gtm2ijqi/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ol2iohhk with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0071306615646824344\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_044217-ol2iohhk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/ol2iohhk' target=\"_blank\">classic-sweep-4</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/ol2iohhk' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/ol2iohhk</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇███████</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▅█▇██▇▇▇</td></tr><tr><td>val_loss</td><td>█▄▃▁▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.64433</td></tr><tr><td>train_loss</td><td>1.1502</td></tr><tr><td>val_accuracy</td><td>0.56188</td></tr><tr><td>val_loss</td><td>1.4729</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">classic-sweep-4</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/ol2iohhk' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/ol2iohhk</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250811_044217-ol2iohhk/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5urv4w9d with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.005812507068956667\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_044950-5urv4w9d</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/5urv4w9d' target=\"_blank\">dauntless-sweep-5</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/5urv4w9d' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/5urv4w9d</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.78461</td></tr><tr><td>train_loss</td><td>0.7041</td></tr><tr><td>val_accuracy</td><td>0.67656</td></tr><tr><td>val_loss</td><td>1.13033</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dauntless-sweep-5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/5urv4w9d' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/5urv4w9d</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250811_044950-5urv4w9d/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: az09bijt with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.007155022857331171\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_045533-az09bijt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/az09bijt' target=\"_blank\">wandering-sweep-6</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/az09bijt' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/az09bijt</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.7529</td></tr><tr><td>train_loss</td><td>0.80252</td></tr><tr><td>val_accuracy</td><td>0.68392</td></tr><tr><td>val_loss</td><td>1.10677</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">wandering-sweep-6</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/az09bijt' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/az09bijt</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250811_045533-az09bijt/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kqw98w3c with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0024613924036533076\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_045944-kqw98w3c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kqw98w3c' target=\"_blank\">floral-sweep-7</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kqw98w3c' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kqw98w3c</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.84783</td></tr><tr><td>train_loss</td><td>0.49896</td></tr><tr><td>val_accuracy</td><td>0.70821</td></tr><tr><td>val_loss</td><td>1.11037</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">floral-sweep-7</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kqw98w3c' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/kqw98w3c</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250811_045944-kqw98w3c/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fzz4hix9 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005545456743751991\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_050306-fzz4hix9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/fzz4hix9' target=\"_blank\">frosty-sweep-8</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/sweeps/2tyssrla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/fzz4hix9' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/fzz4hix9</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.7178</td></tr><tr><td>train_loss</td><td>0.91093</td></tr><tr><td>val_accuracy</td><td>0.64961</td></tr><tr><td>val_loss</td><td>1.16323</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">frosty-sweep-8</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/fzz4hix9' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration/runs/fzz4hix9</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250811_050306-fzz4hix9/logs</code>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Test Data","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport csv\n\n# ---------------- Dataset & Utils ----------------\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(x) for x in inputs]\n    target_lens = [len(x) for x in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\n# ---------------- Models ----------------\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, token, hidden):\n        x = self.embedding(token.unsqueeze(1))\n        output, hidden = self.rnn(x, hidden)\n        output = self.fc(output.squeeze(1))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        hidden = self.encoder(src, src_lens)\n        tgt_len = tgt.size(1)\n        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n        input_token = tgt[:, 0]\n        for t in range(1, tgt_len):\n            output, hidden = self.decoder(input_token, hidden)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            input_token = tgt[:, t] if teacher_force else output.argmax(1)\n        return outputs\n\n# ---------------- Train + Eval ----------------\ndef train_model(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for src, tgt, src_lens, _ in dataloader:\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)\n        loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef evaluate_and_save(model, dataloader, input_vocab, output_vocab, device, csv_path=None):\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    correct = 0\n    total = 0\n    results = []\n\n    with torch.no_grad():\n        for src, tgt, src_lens, _ in dataloader:\n            src = src.to(device)\n            hidden = model.encoder(src, src_lens)\n            input_token = torch.tensor([output_vocab['<sos>']] * src.size(0)).to(device)\n            decoded = []\n            for _ in range(20):\n                output, hidden = model.decoder(input_token, hidden)\n                input_token = output.argmax(1)\n                decoded.append(input_token)\n            decoded = torch.stack(decoded, dim=1)\n\n            for i in range(src.size(0)):\n                pred = ''.join([inv_output_vocab[t.item()] for t in decoded[i] if t.item() not in [output_vocab['<eos>'], 0]])\n                truth = ''.join([inv_output_vocab[t.item()] for t in tgt[i][1:-1]])\n                inp = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n                results.append((inp, pred, truth))\n                if pred == truth:\n                    correct += 1\n                total += 1\n\n    acc = correct / total * 100\n    print(f\"\\n Test Accuracy: {acc:.2f}%\")\n    for inp, pred, truth in results[:10]:\n        print(f\"{inp:<15} | Pred: {pred:<20} | Truth: {truth}\")\n\n    if csv_path is not None:\n        with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n            writer = csv.writer(f)\n            writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n            writer.writerows(results)\n        print(f\"\\n Predictions saved to: {csv_path}\")\n\n    return acc, results\n\n\n# ---------------- Run ----------------\nif __name__ == \"__main__\":\n    config = {\n        \"embed_size\": 128,\n        \"hidden_size\": 256,\n        \"num_layers\": 3,\n        \"cell_type\": \"LSTM\",\n        \"dropout\": 0.1,\n        \"batch_size\": 32,\n        \"lr\": 0.001518,\n        \"epochs\": 5,\n    }\n\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_acc = 0\n    for epoch in range(config[\"epochs\"]):\n        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n        print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n        acc, results = evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=None)\n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n    print(\"\\n Loading best model for final evaluation...\")\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n\n    # Save predictions CSV here\n    evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-10T16:03:10.001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Transformer Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport wandb\nfrom tqdm import tqdm\nimport math\nimport csv\nfrom collections import namedtuple\n\n# ---------------- Data Processing and Utilities ----------------\n\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n        self.unk_in = input_vocab.get('<unk>', 1)\n        self.unk_out = output_vocab.get('<unk>', 3)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab.get(c, self.unk_in) for c in source]\n        target_ids = [self.sos] + [self.output_vocab.get(c, self.unk_out) for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    \n    input_vocab = {c: i + 2 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    input_vocab['<unk>'] = 1\n    \n    output_vocab = {c: i + 4 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3})\n    \n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded\n\n# ---------------- Transformer Specific Components ----------------\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers,\n                 num_decoder_layers, dim_feedforward, dropout):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=0)\n        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model, padding_idx=0)\n        self.positional_encoding = PositionalEncoding(d_model, dropout)\n        \n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.fc_out = nn.Linear(d_model, output_vocab_size)\n        self.output_vocab_size = output_vocab_size\n        self.sos_idx = 1\n        self.eos_idx = 2\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n        src_embedded = self.positional_encoding(self.encoder_embedding(src) * math.sqrt(self.d_model))\n        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt) * math.sqrt(self.d_model))\n        \n        transformer_out = self.transformer(\n            src_embedded, tgt_embedded,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask\n        )\n        \n        output = self.fc_out(transformer_out)\n        return output\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def create_padding_mask(self, seq, pad_idx=0):\n        return (seq == pad_idx)\n\n# ---------------- Training and Evaluation Functions ----------------\n\ndef accuracy(preds, targets, pad_idx=0):\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\n@torch.no_grad()\ndef evaluate_word_accuracy(model, dataloader, device, output_vocab):\n    model.eval()\n    correct_words = 0\n    total_words = 0\n    \n    for src, tgt in tqdm(dataloader, desc=\"Evaluating\"):\n        src, tgt = src.to(device), tgt.to(device)\n\n        src_padding_mask = model.create_padding_mask(src).to(device)\n        batch_size = src.size(0)\n        max_len = 20\n        \n        # Inference loop for the decoder\n        generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n        \n        for t in range(max_len):\n            tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n            tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n            \n            output = model(src, generated_tokens, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask, tgt_padding_mask=tgt_padding_mask)\n            \n            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n            \n            if (next_token == model.eos_idx).all():\n                break\n\n        for i in range(batch_size):\n            pred_seq = generated_tokens[i]\n            target_seq = tgt[i]\n            \n            pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            \n            pred_word = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n            target_word = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n            if torch.equal(pred_word, target_word):\n                correct_words += 1\n            total_words += 1\n            \n    return correct_words / total_words if total_words > 0 else 0.0\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_acc = 0, 0\n    for src, tgt in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        optimizer.zero_grad()\n        \n        src_padding_mask = model.create_padding_mask(src).to(device)\n        tgt_padding_mask = model.create_padding_mask(tgt).to(device)\n        \n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n        \n        output = model(src, tgt_input, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_mask=tgt_mask)\n        \n        loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        acc = accuracy(output, tgt_output)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_acc += acc\n        \n    return total_loss / len(loader), total_acc / len(loader)\n\n# ---------------- Main Function for W&B Sweep ----------------\n\ndef main():\n    import wandb\n    \n    def generate_run_name(config):\n        return f\"transformer_d:{config.d_model}_nhead:{config.nhead}_layers:{config.num_encoder_layers}\"\n\n    wandb.init(project=\"Dakshina-Translitration-Transformer\", config=wandb.config)\n    config = wandb.config\n    wandb.run.name = generate_run_name(config)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n\n    # Build vocab on train + dev pairs for consistency\n    input_vocab, output_vocab = build_vocab(train_pairs + dev_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n\n    model = TransformerModel(\n        input_vocab_size=len(input_vocab),\n        output_vocab_size=len(output_vocab),\n        d_model=config.d_model,\n        nhead=config.nhead,\n        num_encoder_layers=config.num_encoder_layers,\n        num_decoder_layers=config.num_decoder_layers,\n        dim_feedforward=config.dim_feedforward,\n        dropout=config.dropout\n    ).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_dev_acc = 0\n    for epoch in range(10):\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n        dev_acc = evaluate_word_accuracy(model, dev_loader, device, output_vocab)\n        \n        if dev_acc > best_dev_acc:\n            best_dev_acc = dev_acc\n            torch.save(model.state_dict(), 'best_transformer_model.pth')\n            print(f\"Epoch {epoch+1} - New best model saved with dev word accuracy: {best_dev_acc:.4f}\")\n\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"dev_word_accuracy\": dev_acc\n        })\n\nif __name__ == \"__main__\":\n    sweep_config = {\n        \"method\": \"bayes\",\n        \"metric\": {\"name\": \"dev_word_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"d_model\": {\"values\": [128, 256, 512]},\n            \"nhead\": {\"values\": [4, 8, 16]},\n            \"num_encoder_layers\": {\"values\": [2, 4]},\n            \"num_decoder_layers\": {\"values\": [2, 4]},\n            \"dim_feedforward\": {\"values\": [512, 1024, 2048]},\n            \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.001},\n            \"batch_size\": {\"values\": [16, 32, 64]}\n        }\n    }\n    \n    sweep_id = wandb.sweep(sweep_config, project=\"Dakshina-Translitration-Transformer\")\n    wandb.agent(sweep_id, function=main, count=8)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-10T16:03:10.002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport wandb\nfrom tqdm import tqdm\nimport math\nimport csv\nfrom collections import namedtuple\n\n# ---------------- Data Processing and Utilities ----------------\n\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n        # Robustly get unk indices, with fallbacks\n        self.unk_in = input_vocab.get('<unk>', 1) \n        self.unk_out = output_vocab.get('<unk>', 3)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        # Use .get() with unk_in/unk_out for handling unseen characters\n        input_ids = [self.input_vocab.get(c, self.unk_in) for c in source]\n        target_ids = [self.sos] + [self.output_vocab.get(c, self.unk_out) for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    \n    # Vocab indexing: <pad>:0, <unk>:1, then sorted chars\n    input_vocab = {c: i + 2 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    input_vocab['<unk>'] = 1\n    \n    # Vocab indexing: <pad>:0, <sos>:1, <eos>:2, <unk>:3, then sorted chars\n    output_vocab = {c: i + 4 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3})\n    \n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    # Ensure the path is correct for your environment (e.g., Kaggle, local, Colab)\n    # Common issue: FileNotFoundError if path is wrong.\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded\n\n# ---------------- Transformer Specific Components ----------------\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0) # Add batch dimension\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # Add positional encoding to input embeddings\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers,\n                 num_decoder_layers, dim_feedforward, dropout):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=0)\n        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model, padding_idx=0)\n        self.positional_encoding = PositionalEncoding(d_model, dropout)\n        \n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True # Important: Use batch_first for convenience\n        )\n        \n        self.fc_out = nn.Linear(d_model, output_vocab_size)\n        self.output_vocab_size = output_vocab_size\n        self.sos_idx = 1\n        self.eos_idx = 2\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n        # Embed and add positional encoding\n        src_embedded = self.positional_encoding(self.encoder_embedding(src) * math.sqrt(self.d_model))\n        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt) * math.sqrt(self.d_model))\n        \n        # Pass through Transformer layers\n        transformer_out = self.transformer(\n            src_embedded, tgt_embedded,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask\n        )\n        \n        # Linear layer to get vocabulary logits\n        output = self.fc_out(transformer_out)\n        return output\n\n    def generate_square_subsequent_mask(self, sz):\n        # Generates a mask to prevent attention to future tokens in the decoder\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def create_padding_mask(self, seq, pad_idx=0):\n        # Generates a boolean mask for padding tokens\n        return (seq == pad_idx)\n\n# ---------------- Training and Evaluation Functions ----------------\n\ndef accuracy(preds, targets, pad_idx=0):\n    # Calculates character-level accuracy, ignoring padding\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\n@torch.no_grad()\ndef evaluate_word_accuracy(model, dataloader, device, output_vocab):\n    model.eval()\n    correct_words = 0\n    total_words = 0\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    \n    for src, tgt in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        src_padding_mask = model.create_padding_mask(src).to(device)\n        batch_size = src.size(0)\n        max_len = 20 # Max length for generated output (could be dynamically set based on input length if needed)\n        \n        # Initialize decoder input with <sos> tokens for greedy decoding\n        generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n        \n        for t in range(max_len):\n            # Create masks for the current generated sequence length\n            tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n            tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n            \n            # Forward pass to get next token predictions\n            output = model(src, generated_tokens, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask, tgt_padding_mask=tgt_padding_mask)\n            \n            # Get the token with the highest probability\n            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n            \n            # Append the predicted token to the generated sequence\n            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n            \n            # Stop if all sequences in the batch have generated the <eos> token\n            if (next_token == model.eos_idx).all():\n                break\n\n        # Calculate word-level accuracy\n        for i in range(batch_size):\n            pred_seq = generated_tokens[i]\n            target_seq = tgt[i]\n            \n            # Find the first <eos> token to trim the sequence (excluding <sos> and <eos> itself)\n            pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            \n            # Extract the actual word tokens, excluding <sos> and <eos>\n            pred_word = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n            target_word = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n            if torch.equal(pred_word, target_word):\n                correct_words += 1\n            total_words += 1\n            \n    return correct_words / total_words if total_words > 0 else 0.0\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_char_acc = 0, 0\n    for src, tgt in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n\n        optimizer.zero_grad()\n        \n        src_padding_mask = model.create_padding_mask(src).to(device)\n        \n        tgt_input = tgt[:, :-1] # Input for decoder, excludes the last token\n        tgt_output = tgt[:, 1:]  # Target for loss, excludes the first token (<sos>)\n\n        # ************ CRITICAL FIX ************\n        # Create tgt_padding_mask from tgt_input to match its length\n        tgt_padding_mask = model.create_padding_mask(tgt_input).to(device)\n       \n        \n        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n        \n        output = model(src, tgt_input, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_mask=tgt_mask)\n        \n        # Reshape output and target for CrossEntropyLoss\n        loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        char_acc = accuracy(output, tgt_output) # Character-level accuracy\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_char_acc += char_acc\n        \n    return total_loss / len(loader), total_char_acc / len(loader)\n\ndef generate_predictions_csv(model, dataloader, input_vocab, output_vocab, device, csv_path):\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    results = []\n\n    with torch.no_grad():\n        for src, tgt in tqdm(dataloader, desc=\"Generating Test Predictions\"):\n            src = src.to(device)\n            batch_size = src.size(0)\n            max_len = 20 # Max length for generated output\n\n            # Inference loop for the decoder (similar to evaluate_word_accuracy)\n            generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n            \n            for t in range(max_len):\n                tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n                tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n                \n                output = model(src, generated_tokens, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask, tgt_padding_mask=tgt_padding_mask)\n                \n                next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n                generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n                \n                if (next_token == model.eos_idx).all():\n                    break\n\n            for i in range(batch_size):\n                pred_seq = generated_tokens[i]\n                target_seq = tgt[i]\n                \n                pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n                target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n                \n                pred_word_tokens = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n                # Ensure truth_word_tokens also excludes any potential padding if it's shorter than predicted length\n                truth_word_tokens = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n                pred_str = ''.join([inv_output_vocab[t.item()] for t in pred_word_tokens if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                truth_str = ''.join([inv_output_vocab[t.item()] for t in truth_word_tokens if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n                inp_str = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n                results.append((inp_str, pred_str, truth_str))\n    \n    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n        writer.writerows(results)\n    print(f\"\\nPredictions saved to: {csv_path}\")\n\n# ---------------- Main Function for W&B Sweep ----------------\n\ndef main():\n    import wandb\n    \n    def generate_run_name(config):\n        return f\"transformer_d:{config.d_model}_nhead:{config.nhead}_layers:{config.num_encoder_layers}\"\n\n    wandb.init(project=\"Dakshina-Translitration-Transformer\", config=wandb.config)\n    config = wandb.config\n    wandb.run.name = generate_run_name(config)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    # *******************************************************************\n\n    # Build vocab on train + dev pairs for consistency\n    input_vocab, output_vocab = build_vocab(train_pairs + dev_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab) # Prepare test dataset here\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn) # Batch size 1 for individual prediction\n\n    model = TransformerModel(\n        input_vocab_size=len(input_vocab),\n        output_vocab_size=len(output_vocab),\n        d_model=config.d_model,\n        nhead=config.nhead,\n        num_encoder_layers=config.num_encoder_layers,\n        num_decoder_layers=config.num_decoder_layers,\n        dim_feedforward=config.dim_feedforward,\n        dropout=config.dropout\n    ).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=0) # ignore_index=0 for <pad> token\n\n    best_dev_acc = 0\n    # Training loop\n    for epoch in range(10): # Looping for 10 epochs\n        train_loss, train_char_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n        dev_word_acc = evaluate_word_accuracy(model, dev_loader, device, output_vocab)\n        \n        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Char Acc: {train_char_acc:.4f} | Dev Word Acc: {dev_word_acc:.4f}\")\n        \n        if dev_word_acc > best_dev_acc:\n            best_dev_acc = dev_word_acc\n            torch.save(model.state_dict(), 'best_transformer_model.pth')\n            print(f\" -> New best model saved with dev word accuracy: {best_dev_acc:.4f}\")\n\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_char_accuracy\": train_char_acc,\n            \"dev_word_accuracy\": dev_word_acc\n        })\n\n    print(\"\\nTraining complete. Loading best model for final evaluation on test set...\")\n    # Load the best model found during training\n    try:\n        model.load_state_dict(torch.load('best_transformer_model.pth'))\n    except FileNotFoundError:\n        print(\"Error: 'best_transformer_model.pth' not found. Ensure training completed successfully and model was saved.\")\n        return # Exit main if model not found\n\n    # Final evaluation on the test set (using the best saved model)\n    final_test_word_acc = evaluate_word_accuracy(model, test_loader, device, output_vocab)\n    print(f\"\\n--- Final Test Set Evaluation Results ---\")\n    print(f\"Word-level Accuracy on Test Set: {final_test_word_acc:.4f}\")\n    \n    # Generate and save predictions to CSV using the best model\n    generate_predictions_csv(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")\n    print(\"Test predictions saved to test_predictions.csv\")\n\n\nif __name__ == \"__main__\":\n    # Define your W&B sweep configuration\n    sweep_config = {\n        \"method\": \"bayes\", # Bayesian optimization\n        \"metric\": {\"name\": \"dev_word_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"d_model\": {\"values\": [128, 256, 512]},\n            \"nhead\": {\"values\": [4, 8, 16]},\n            \"num_encoder_layers\": {\"values\": [2, 4]},\n            \"num_decoder_layers\": {\"values\": [2, 4]},\n            \"dim_feedforward\": {\"values\": [512, 1024, 2048]},\n            \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.001},\n            \"batch_size\": {\"values\": [16, 32, 64]}\n        }\n    }\n    \n    # Initialize and run the W&B agent\n    sweep_id = wandb.sweep(sweep_config, project=\"Dakshina-Translitration-Transformer\")\n    wandb.agent(sweep_id, function=main, count=1) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T16:05:29.321033Z","iopub.execute_input":"2025-08-10T16:05:29.321768Z","iopub.status.idle":"2025-08-10T16:23:40.942805Z","shell.execute_reply.started":"2025-08-10T16:05:29.321745Z","shell.execute_reply":"2025-08-10T16:23:40.942288Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: 8f8jt52p\nSweep URL: https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/8f8jt52p\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fo2kopz1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \td_model: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_feedforward: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.000811166346844853\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnhead: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration-Transformer' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250810_160542-fo2kopz1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/fo2kopz1' target=\"_blank\">good-sweep-1</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/8f8jt52p' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/8f8jt52p</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/8f8jt52p' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/8f8jt52p</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/fo2kopz1' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/fo2kopz1</a>"},"metadata":{}},{"name":"stderr","text":"Training:   0%|          | 0/2763 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\nEvaluating:   0%|          | 0/273 [00:00<?, ?it/s]          /usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n  output = torch._nested_tensor_from_mask(\n                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 1.7516 | Train Char Acc: 0.4518 | Dev Word Acc: 0.0124\n -> New best model saved with dev word accuracy: 0.0124\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 1.3942 | Train Char Acc: 0.5379 | Dev Word Acc: 0.0138\n -> New best model saved with dev word accuracy: 0.0138\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 1.2984 | Train Char Acc: 0.5646 | Dev Word Acc: 0.0138\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 1.2414 | Train Char Acc: 0.5827 | Dev Word Acc: 0.0225\n -> New best model saved with dev word accuracy: 0.0225\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 1.1993 | Train Char Acc: 0.5957 | Dev Word Acc: 0.0209\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 1.1722 | Train Char Acc: 0.6048 | Dev Word Acc: 0.0184\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 1.1457 | Train Char Acc: 0.6128 | Dev Word Acc: 0.0225\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 1.1284 | Train Char Acc: 0.6183 | Dev Word Acc: 0.0264\n -> New best model saved with dev word accuracy: 0.0264\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 1.1111 | Train Char Acc: 0.6243 | Dev Word Acc: 0.0229\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 1.0970 | Train Char Acc: 0.6294 | Dev Word Acc: 0.0287\n -> New best model saved with dev word accuracy: 0.0287\n\nTraining complete. Loading best model for final evaluation on test set...\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"\n--- Final Test Set Evaluation Results ---\nWord-level Accuracy on Test Set: 0.0262\n","output_type":"stream"},{"name":"stderr","text":"Generating Test Predictions:   0%|          | 0/4502 [00:00<?, ?it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dev_word_accuracy</td><td>▁▂▂▅▅▄▅▇▆█</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_char_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dev_word_accuracy</td><td>0.02868</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>train_char_accuracy</td><td>0.62936</td></tr><tr><td>train_loss</td><td>1.09698</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">transformer_d:128_nhead:4_layers:4</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/fo2kopz1' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/fo2kopz1</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250810_160542-fo2kopz1/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run fo2kopz1 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 302, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_36/2560327593.py\", line 361, in main\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     generate_predictions_csv(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_36/2560327593.py\", line 254, in generate_predictions_csv\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = model(src, generated_tokens, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask, tgt_padding_mask=tgt_padding_mask)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                                            ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'src_padding_mask' is not defined\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## For Test data","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport pandas as pd\nimport csv\nfrom collections import namedtuple\nimport math\n\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n        # Robustly get unk indices, with fallbacks\n        self.unk_in = input_vocab.get('<unk>', 1) \n        self.unk_out = output_vocab.get('<unk>', 3)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        # Use .get() with unk_in/unk_out for handling unseen characters\n        input_ids = [self.input_vocab.get(c, self.unk_in) for c in source]\n        target_ids = [self.sos] + [self.output_vocab.get(c, self.unk_out) for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    \n    # Vocab indexing: <pad>:0, <unk>:1, then sorted chars\n    input_vocab = {c: i + 2 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    input_vocab['<unk>'] = 1\n    \n    # Vocab indexing: <pad>:0, <sos>:1, <eos>:2, <unk>:3, then sorted chars\n    output_vocab = {c: i + 4 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3})\n    \n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    # Ensure the path is correct for your environment (e.g., Kaggle, local, Colab)\n    # Common issue: FileNotFoundError if path is wrong.\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded\n\n# ---------------- Transformer Specific Components ----------------\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0) # Add batch dimension\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # Add positional encoding to input embeddings\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers,\n                 num_decoder_layers, dim_feedforward, dropout):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=0)\n        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model, padding_idx=0)\n        self.positional_encoding = PositionalEncoding(d_model, dropout)\n        \n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True # Important: Use batch_first for convenience\n        )\n        \n        self.fc_out = nn.Linear(d_model, output_vocab_size)\n        self.output_vocab_size = output_vocab_size\n        self.sos_idx = 1\n        self.eos_idx = 2\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n        # Embed and add positional encoding\n        src_embedded = self.positional_encoding(self.encoder_embedding(src) * math.sqrt(self.d_model))\n        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt) * math.sqrt(self.d_model))\n        \n        # Pass through Transformer layers\n        transformer_out = self.transformer(\n            src_embedded, tgt_embedded,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask\n        )\n        \n        # Linear layer to get vocabulary logits\n        output = self.fc_out(transformer_out)\n        return output\n\n    def generate_square_subsequent_mask(self, sz):\n        # Generates a mask to prevent attention to future tokens in the decoder\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def create_padding_mask(self, seq, pad_idx=0):\n        # Generates a boolean mask for padding tokens\n        return (seq == pad_idx)\n\n# ----------------- Evaluation Function for Final Test Set -----------------\n@torch.no_grad()\ndef evaluate_test_set(model, test_loader, device, input_vocab, output_vocab): # <-- Corrected function signature\n    model.eval()\n    correct_words = 0\n    total_words = 0\n    results = []\n    \n    # Create inverse vocab from the correct dictionaries\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    \n    for src, tgt in tqdm(test_loader, desc=\"Evaluating Test Set\"):\n        src, tgt = src.to(device), tgt.to(device)\n\n        src_padding_mask = model.create_padding_mask(src).to(device)\n        batch_size = src.size(0)\n        max_len = 20\n        \n        generated_tokens = torch.full((batch_size, 1), model.sos_idx, dtype=torch.long, device=device)\n        \n        for t in range(max_len):\n            tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1)).to(device)\n            tgt_padding_mask = model.create_padding_mask(generated_tokens).to(device)\n            \n            output = model(src, generated_tokens, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask, tgt_padding_mask=tgt_padding_mask)\n            \n            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n            \n            if (next_token == model.eos_idx).all():\n                break\n\n        for i in range(batch_size):\n            pred_seq = generated_tokens[i]\n            target_seq = tgt[i]\n            \n            pred_end = (pred_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            target_end = (target_seq == model.eos_idx).nonzero(as_tuple=True)[0]\n            \n            pred_word = pred_seq[1:pred_end[0] if pred_end.numel() > 0 else len(pred_seq)]\n            target_word = target_seq[1:target_end[0] if target_end.numel() > 0 else len(target_seq)]\n\n            # Filter out padding tokens for string conversion\n            pred_str = ''.join([inv_output_vocab.get(t.item(), '<unk>') for t in pred_word if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n            truth_str = ''.join([inv_output_vocab.get(t.item(), '<unk>') for t in target_word if t.item() not in [model.sos_idx, model.eos_idx, 0]])\n            inp_str = ''.join([inv_input_vocab.get(t.item(), '<unk>') for t in src[i] if t.item() not in [0]]) # src only has pad\n            \n            results.append((inp_str, pred_str, truth_str))\n\n            if torch.equal(pred_word, target_word):\n                correct_words += 1\n            total_words += 1\n            \n    acc = correct_words / total_words if total_words > 0 else 0.0\n    \n    # Save predictions to CSV\n    with open(\"test_predictions.csv\", mode='w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n        writer.writerows(results)\n    \n    return acc\n\n# ---------------- Main Function for W&B Sweep ----------------\n\ndef main():\n    import wandb\n    \n    def generate_run_name(config):\n        return f\"transformer_d:{config.d_model}_nhead:{config.nhead}_layers:{config.num_encoder_layers}\"\n\n    wandb.init(project=\"Dakshina-Translitration-Transformer\", config=wandb.config)\n    config = wandb.config\n    wandb.run.name = generate_run_name(config)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ************ IMPORTANT: CORRECT YOUR DATASET PATHS ************\n    # The FileNotFoundError indicates these paths are incorrect for your environment.\n    # Update these paths to where your 'hi.translit.sampled.train.tsv', 'dev.tsv', and 'test.tsv'\n    # files are actually located.\n    # Example for local run if files are in the same directory as script:\n    # train_pairs = load_pairs(\"hi.translit.sampled.train.tsv\")\n    # dev_pairs = load_pairs(\"hi.translit.sampled.dev.tsv\")\n    # test_pairs = load_pairs(\"hi.translit.sampled.test.tsv\")\n    # Example for Kaggle (if different from what you currently have):\n    # train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset-v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    # dev_pairs = load_pairs(\"/kaggle/input/dakshina-dataset-v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n    # test_pairs = load_pairs(\"/kaggle/input/dakshina-dataset-v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    # You need to ensure the exact path exists.\n    train_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    # *******************************************************************\n\n    # Build vocab on train + dev pairs for consistency\n    input_vocab, output_vocab = build_vocab(train_pairs + dev_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab) # Prepare test dataset here\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn) # Batch size 1 for individual prediction\n\n    model = TransformerModel(\n        input_vocab_size=len(input_vocab),\n        output_vocab_size=len(output_vocab),\n        d_model=config.d_model,\n        nhead=config.nhead,\n        num_encoder_layers=config.num_encoder_layers,\n        num_decoder_layers=config.num_decoder_layers,\n        dim_feedforward=config.dim_feedforward,\n        dropout=config.dropout\n    ).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=0) # ignore_index=0 for <pad> token\n\n    best_dev_acc = 0\n    # Training loop\n    for epoch in range(10): # Looping for 10 epochs\n        train_loss, train_char_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n        dev_word_acc = evaluate_word_accuracy(model, dev_loader, device, output_vocab)\n        \n        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Char Acc: {train_char_acc:.4f} | Dev Word Acc: {dev_word_acc:.4f}\")\n        \n        if dev_word_acc > best_dev_acc:\n            best_dev_acc = dev_word_acc\n            torch.save(model.state_dict(), 'best_transformer_model.pth')\n            print(f\" -> New best model saved with dev word accuracy: {best_dev_acc:.4f}\")\n\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_char_accuracy\": train_char_acc,\n            \"dev_word_accuracy\": dev_word_acc\n        })\n\n    print(\"\\nTraining complete. Loading best model for final evaluation on test set...\")\n    # Load the best model found during training\n    try:\n        model.load_state_dict(torch.load('best_transformer_model.pth'))\n    except FileNotFoundError:\n        print(\"Error: 'best_transformer_model.pth' not found. Ensure training completed successfully and model was saved.\")\n        return # Exit main if model not found\n\n    # Final evaluation on the test set (using the best saved model)\n    # ************ CRITICAL CORRECTION ************\n    # Pass input_vocab to the evaluation function\n    final_test_word_acc = evaluate_test_set(model, test_loader, device, input_vocab, output_vocab) \n    # *********************************************\n    print(f\"\\n--- Final Test Set Evaluation Results ---\")\n    print(f\"Word-level Accuracy on Test Set: {final_test_word_acc:.4f}\")\n    \n    # Generate and save predictions to CSV using the best model\n    generate_predictions_csv(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")\n    print(\"Test predictions saved to test_predictions.csv\")\n\n\nif __name__ == \"__main__\": # <-- Changed back to \"__main__\"\n    # Define your W&B sweep configuration\n    sweep_config = {\n        \"method\": \"bayes\", # Bayesian optimization\n        \"metric\": {\"name\": \"dev_word_accuracy\", \"goal\": \"maximize\"}, # Metric to optimize\n        \"parameters\": {\n            \"d_model\": {\"values\": [128, 256, 512]},\n            \"nhead\": {\"values\": [4, 8, 16]},\n            \"num_encoder_layers\": {\"values\": [2, 4]},\n            \"num_decoder_layers\": {\"values\": [2, 4]},\n            \"dim_feedforward\": {\"values\": [512, 1024, 2048]},\n            \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.001},\n            \"batch_size\": {\"values\": [16, 32, 64]}\n        }\n    }\n    \n    # Initialize and run the W&B agent\n    sweep_id = wandb.sweep(sweep_config, project=\"Dakshina-Translitration-Transformer\")\n    wandb.agent(sweep_id, function=main, count=2) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T16:43:02.657616Z","iopub.execute_input":"2025-08-10T16:43:02.657921Z","execution_failed":"2025-08-10T17:06:27.103Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: 0bjtuwyd\nSweep URL: https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4pz3n5u0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \td_model: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_feedforward: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0007956222571568462\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnhead: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration-Transformer' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250810_164309-4pz3n5u0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/4pz3n5u0' target=\"_blank\">pleasant-sweep-1</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/4pz3n5u0' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/4pz3n5u0</a>"},"metadata":{}},{"name":"stderr","text":"Training:   0%|          | 0/2763 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 3.3934 | Train Char Acc: 0.1350 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 3.4225 | Train Char Acc: 0.1384 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 3.4224 | Train Char Acc: 0.1384 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 3.4222 | Train Char Acc: 0.1384 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 3.4223 | Train Char Acc: 0.1385 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 3.3772 | Train Char Acc: 0.1447 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 3.3512 | Train Char Acc: 0.1495 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 3.3423 | Train Char Acc: 0.1509 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 3.3392 | Train Char Acc: 0.1522 | Dev Word Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 3.3340 | Train Char Acc: 0.1525 | Dev Word Acc: 0.0000\n\nTraining complete. Loading best model for final evaluation on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dev_word_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_char_accuracy</td><td>▁▂▂▂▂▅▇▇██</td></tr><tr><td>train_loss</td><td>▆████▄▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dev_word_accuracy</td><td>0</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>train_char_accuracy</td><td>0.15251</td></tr><tr><td>train_loss</td><td>3.33401</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">transformer_d:512_nhead:8_layers:4</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/4pz3n5u0' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/4pz3n5u0</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250810_164309-4pz3n5u0/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4pz3n5u0 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 302, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_36/458573833.py\", line 277, in main\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model.load_state_dict(torch.load('best_transformer_model.pth'))\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 2581, in load_state_dict\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise RuntimeError(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Error(s) in loading state_dict for TransformerModel:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tMissing key(s) in state_dict: \"transformer.decoder.layers.2.self_attn.in_proj_weight\", \"transformer.decoder.layers.2.self_attn.in_proj_bias\", \"transformer.decoder.layers.2.self_attn.out_proj.weight\", \"transformer.decoder.layers.2.self_attn.out_proj.bias\", \"transformer.decoder.layers.2.multihead_attn.in_proj_weight\", \"transformer.decoder.layers.2.multihead_attn.in_proj_bias\", \"transformer.decoder.layers.2.multihead_attn.out_proj.weight\", \"transformer.decoder.layers.2.multihead_attn.out_proj.bias\", \"transformer.decoder.layers.2.linear1.weight\", \"transformer.decoder.layers.2.linear1.bias\", \"transformer.decoder.layers.2.linear2.weight\", \"transformer.decoder.layers.2.linear2.bias\", \"transformer.decoder.layers.2.norm1.weight\", \"transformer.decoder.layers.2.norm1.bias\", \"transformer.decoder.layers.2.norm2.weight\", \"transformer.decoder.layers.2.norm2.bias\", \"transformer.decoder.layers.2.norm3.weight\", \"transformer.decoder.layers.2.norm3.bias\", \"transformer.decoder.layers.3.self_attn.in_proj_weight\", \"transformer.decoder.layers.3.self_attn.in_proj_bias\", \"transformer.decoder.layers.3.self_attn.out_proj.weight\", \"transformer.decoder.layers.3.self_attn.out_proj.bias\", \"transformer.decoder.layers.3.multihead_attn.in_proj_weight\", \"transformer.decoder.layers.3.multihead_attn.in_proj_bias\", \"transformer.decoder.layers.3.multihead_attn.out_proj.weight\", \"transformer.decoder.layers.3.multihead_attn.out_proj.bias\", \"transformer.decoder.layers.3.linear1.weight\", \"transformer.decoder.layers.3.linear1.bias\", \"transformer.decoder.layers.3.linear2.weight\", \"transformer.decoder.layers.3.linear2.bias\", \"transformer.decoder.layers.3.norm1.weight\", \"transformer.decoder.layers.3.norm1.bias\", \"transformer.decoder.layers.3.norm2.weight\", \"transformer.decoder.layers.3.norm2.bias\", \"transformer.decoder.layers.3.norm3.weight\", \"transformer.decoder.layers.3.norm3.bias\". \n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for encoder_embedding.weight: copying a param with shape torch.Size([28, 128]) from checkpoint, the shape in current model is torch.Size([28, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for decoder_embedding.weight: copying a param with shape torch.Size([67, 128]) from checkpoint, the shape in current model is torch.Size([67, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for positional_encoding.pe: copying a param with shape torch.Size([1, 5000, 128]) from checkpoint, the shape in current model is torch.Size([1, 5000, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([1536, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([1536, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([1536, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([1536, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.layers.3.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.encoder.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([1536, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([1536, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([1536, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([1536, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.norm3.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.layers.1.norm3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for transformer.decoder.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \tsize mismatch for fc_out.weight: copying a param with shape torch.Size([67, 128]) from checkpoint, the shape in current model is torch.Size([67, 512]).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 283rhr4u with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \td_model: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_feedforward: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0004184066860157314\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnhead: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Dakshina-Translitration-Transformer' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250810_170228-283rhr4u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/283rhr4u' target=\"_blank\">misty-sweep-2</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/sweeps/0bjtuwyd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/283rhr4u' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/Dakshina-Translitration-Transformer/runs/283rhr4u</a>"},"metadata":{}},{"name":"stderr","text":"Training:   0%|          | 0/1382 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 2.1521 | Train Char Acc: 0.3538 | Dev Word Acc: 0.0030\n -> New best model saved with dev word accuracy: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 1.7054 | Train Char Acc: 0.4467 | Dev Word Acc: 0.0106\n -> New best model saved with dev word accuracy: 0.0106\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 1.5835 | Train Char Acc: 0.4797 | Dev Word Acc: 0.0117\n -> New best model saved with dev word accuracy: 0.0117\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 1.5149 | Train Char Acc: 0.4981 | Dev Word Acc: 0.0149\n -> New best model saved with dev word accuracy: 0.0149\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 1.4700 | Train Char Acc: 0.5105 | Dev Word Acc: 0.0172\n -> New best model saved with dev word accuracy: 0.0172\n","output_type":"stream"},{"name":"stderr","text":"Training:   5%|▍         | 65/1382 [00:01<00:27, 47.60it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}